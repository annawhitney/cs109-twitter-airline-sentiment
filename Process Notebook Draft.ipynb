{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Twitter Feeds to Find Your Favorite Airline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import re\n",
    "from pattern.en import parse\n",
    "from pattern.en import pprint\n",
    "from pattern.vector import stem, PORTER, LEMMA\n",
    "from sklearn.feature_extraction import text\n",
    "from gensim import corpora "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/opt/apache-spark/libexec\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "print findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# adapted from HW5\n",
    "def get_parts(thetext, punctuation):\n",
    "    # generate stopwords list & regexes for 2+ periods or 2+ dashes\n",
    "    stop = text.ENGLISH_STOP_WORDS\n",
    "    regex1=re.compile(r\"\\.{2,}\")\n",
    "    regex2=re.compile(r\"\\-{2,}\")\n",
    "    thetext=re.sub(regex1, ' ', thetext)\n",
    "    thetext=re.sub(regex2, ' ', thetext)\n",
    "    nouns=[]\n",
    "    descriptives=[]\n",
    "    for i,sentence in enumerate(parse(thetext, tokenize=True, lemmata=True).split()):\n",
    "        nouns.append([])\n",
    "        descriptives.append([])\n",
    "        for token in sentence:\n",
    "            if len(token[4]) >0:\n",
    "                if token[1] in ['JJ', 'JJR', 'JJS']:\n",
    "                    if token[4] in stop or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    descriptives[i].append(token[4])\n",
    "                elif token[1] in ['NN', 'NNS']:\n",
    "                    if token[4] in stop or token[4][0] in punctuation or token[4][-1] in punctuation or len(token[4])==1:\n",
    "                        continue\n",
    "                    nouns[i].append(token[4])\n",
    "    out=zip(nouns, descriptives)\n",
    "    nouns2=[]\n",
    "    descriptives2=[]\n",
    "    for n,d in out:\n",
    "        if len(n)!=0 and len(d)!=0:\n",
    "            nouns2.append(n)\n",
    "            descriptives2.append(d)\n",
    "    return nouns2, descriptives2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# initialize Spark context\n",
    "conf = pyspark.SparkConf().setAppName(\"Twitter_Airline\").setMaster(\"local[*]\")\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "\n",
    "# get tweets from text file\n",
    "# using sample tweets for now\n",
    "text_lines = sc.textFile('sample_tweets.json')\n",
    "tweets = text_lines.map(json.loads)\n",
    "tweets_text = tweets.map(lambda t: t['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment of a sentence based on log probs in a word list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read the word list file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# read the word list\n",
    "def readSentimentList(file_name):\n",
    "    ifile = open(file_name, 'r')\n",
    "    happy_log_probs = {}\n",
    "    sad_log_probs = {}\n",
    "    ifile.readline() #Ignore title row\n",
    "    # splitting the csv\n",
    "    for line in ifile:\n",
    "        tokens = line[:-1].split(',')\n",
    "        happy_log_probs[tokens[0]] = float(tokens[1])\n",
    "        sad_log_probs[tokens[0]] = float(tokens[2])\n",
    "\n",
    "    return happy_log_probs, sad_log_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Naive Bayes rule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifySentiment(words, happy_log_probs, sad_log_probs):\n",
    "    # get the log-probability of each word under each sentiment\n",
    "    happy_probs = [happy_log_probs[word] for word in words if word in happy_log_probs]\n",
    "    sad_probs = [sad_log_probs[word] for word in words if word in sad_log_probs]\n",
    "\n",
    "    # sum all the log-probabilities for each sentiment to get a log-probability for the whole tweet\n",
    "    tweet_happy_log_prob = np.sum(happy_probs)\n",
    "    tweet_sad_log_prob = np.sum(sad_probs)\n",
    "\n",
    "    # calculate the probability of the tweet belonging to each sentiment\n",
    "    prob_happy = np.reciprocal(np.exp(tweet_sad_log_prob - tweet_happy_log_prob) + 1)\n",
    "    prob_sad = 1 - prob_happy\n",
    "\n",
    "    return prob_happy, prob_sad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the word list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load list of words and log probs\n",
    "happy_log_probs, sad_log_probs = readSentimentList('wordlist.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading in the tweet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['my', 'hate', 'southwest']\n",
      "happy probability:  0.280105168408\n",
      "sad probability: 0.719894831592\n"
     ]
    }
   ],
   "source": [
    "# read tweet\n",
    "tweet1 = ['my', 'hate', 'southwest']\n",
    "\n",
    "# calculate the probability\n",
    "tweet1_happy_prob, tweet1_sad_prob = classifySentiment(tweet1, happy_log_probs, sad_log_probs)\n",
    "\n",
    "print tweet1 \n",
    "print \"happy probability: \" , tweet1_happy_prob \n",
    "print \"sad probability:\", tweet1_sad_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# get words out for sentiment analysis\n",
    "punc = list('.,;:!?()[]{}`''\\\"@#$^&*+-|=~_')\n",
    "sentiment_words = tweets_text.map(lambda t: t.strip(punc).split())\n",
    "\n",
    "# classify sentiment of tweet\n",
    "tweets_probs = sentiment_words.map(lambda ws: classifySentiment(ws, happy_log_probs, sad_log_probs))\n",
    "happy_probs = tweets_probs.keys()\n",
    "sad_probs = tweets_probs.values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA on nouns for topic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# parse tweets to nouns & adjectives\n",
    "tweets_n_a = tweets_text.map(lambda t: get_parts(t, punc))\n",
    "tweets_nouns = tweets_n_a.keys()\n",
    "all_nouns = tweets_nouns.flatMap(lambda l: l).toLocalIterator()\n",
    "tweets_adjs = tweets_n_a.values()\n",
    "\n",
    "# feed nouns into gensim\n",
    "dictionary = corpora.Dictionary(all_nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
